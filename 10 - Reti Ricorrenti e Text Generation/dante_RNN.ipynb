{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dante_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/9%20-%20Reti%20Ricorrenti%20e%20Text%20Generation/dante_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4MjpjKzAue78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generare testo con le Reti Ricorrenti (LSTM)\n",
        "In questo notebook vederemo come è possibile utilizzare le reti neurali ricorrenti LSTM non solamente per classificare del testo ma anche per generarlo ! Quello che andremo a fare è cercare di generare del nuovo testo con lo stesso stile che ha utilizzato Dante Alighieri per scrivere la Divina Commedia.<br><br>\n",
        "Cominciamo scaricando una copia gratuita in TXT della Divina Commedia, puoi otterla da [questo sito internet](https://www.liberliber.it/online/autori/autori-a/dante-alighieri/la-divina-commedia-edizione-petrocchi/), se utilizzi Google Colaboratory o hai wget installato esegui pure il comando qui sotto per scaricare il file."
      ]
    },
    {
      "metadata": {
        "id": "q3cb6BFDHJ44",
        "colab_type": "code",
        "outputId": "9bbfd175-5933-489a-e01b-318326e9e7ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.liberliber.it/mediateca/libri/a/alighieri/la_divina_commedia/txt/la_divin.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-25 11:01:42--  https://www.liberliber.it/mediateca/libri/a/alighieri/la_divina_commedia/txt/la_divin.zip\n",
            "Resolving www.liberliber.it (www.liberliber.it)... 93.186.244.67\n",
            "Connecting to www.liberliber.it (www.liberliber.it)|93.186.244.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 232691 (227K) [application/zip]\n",
            "Saving to: ‘la_divin.zip’\n",
            "\n",
            "\rla_divin.zip          0%[                    ]       0  --.-KB/s               \rla_divin.zip        100%[===================>] 227.24K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-04-25 11:01:42 (2.45 MB/s) - ‘la_divin.zip’ saved [232691/232691]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-bnyH7aNxWV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ed estrai lo zip."
      ]
    },
    {
      "metadata": {
        "id": "_kJMMCZmIgvK",
        "colab_type": "code",
        "outputId": "93333cee-6424-4464-a0f1-700bfcb15b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip la_divin.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  la_divin.zip\n",
            "  inflating: la_divin.txt            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B6knMxL-xcTO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Processiamo i dati\n",
        "Apriamo il file appena scaricato, leggiamone il contenuto e stampiamo i primi 100 caratteri."
      ]
    },
    {
      "metadata": {
        "id": "r6Jb2lDdJ4rJ",
        "colab_type": "code",
        "outputId": "0d092406-8a16-4486-8d15-824b2a09bdd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "with open(\"la_divin.txt\", encoding=\"latin-1\") as divine_file:\n",
        "  divine_txt = divine_file.read()\n",
        "  \n",
        "print(divine_txt[:100])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dante Alighieri\n",
            "La Divina Commedia\n",
            "\n",
            "Questo e-book è stato realizzato anche grazie al sostegno di:\n",
            "E-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dn_cBlzg9PDm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi l'ebook contiene del testo che non ci interessa, usiamo il metodo *.find(text)* per trovare dove inizia e finisce la divina commedia ed eseguiamo lo slicing per tenere soltanto il testo scritto da Dante."
      ]
    },
    {
      "metadata": {
        "id": "l8fKWhRlKLPR",
        "colab_type": "code",
        "outputId": "4601fe21-799a-445c-fa9a-31f8f625ca1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "start = divine_txt.find(\"Nel mezzo del cammin di nostra vita\")\n",
        "end = divine_txt.find(\"l'amor che move il sole e l'altre stelle.\")\n",
        "\n",
        "divine_txt = divine_txt[start:end]\n",
        "\n",
        "print(divine_txt[:100])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "ché la diritta via era smarrit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qLp-hGs090pc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ogni canto contiene una piccola introduzione, ad esempio il primo:\n",
        "<br><br>\n",
        "LA DIVINA COMMEDIA\n",
        "<br>\n",
        "di Dante Alighieri\n",
        "<br>\n",
        "<br>\n",
        "INFERNO\n",
        "<br>\n",
        "<br>\n",
        "CANTO I\n",
        "<br>\n",
        "[Incomincia la Comedia di Dante Alleghieri di Fiorenza, ne la quale tratta de le pene e punimenti de' vizi e de' meriti e premi de le virt˘. Comincia il canto primo de la prima parte la quale si chiama Inferno, nel qual l'auttore fa proemio a tutta l'opera.]<br>\n",
        "<br><br>\n",
        "Il pattern è uguale per ogni canto, quindi possiamo rimuoverlo con un po' di codice:\n",
        " - Usiamo un'espressione regolare per rimuovere tutte le parole che cominciano con almeno due lettere maiscuole.\n",
        " - Usiamo un'altra espressione regolare per rimuovere tutte le frasi contenute tra parentesi quadre.\n",
        " - Rimuoviamo ogni occorrenza della frase 'di Dante Alighieri' dal testo.\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "R_WfHALrSGQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "divine_txt = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", divine_txt)\n",
        "divine_txt = re.sub(\"[A-Z][A-Z]+\",\"\",divine_txt)\n",
        "\n",
        "divine_txt = divine_txt.replace(\"di Dante Alighieri\",\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0e7aDtoP-LF6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Usiamo la solita espressione regolare per rimuovere la punteggiatura, poi rimuoviamo anche i caratteri di 'a capo' e convertiamo tutto il testo in minuscolo."
      ]
    },
    {
      "metadata": {
        "id": "yUPTo6T8_jBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "divine_txt = re.sub(r'[^\\w\\s]','',divine_txt)\n",
        "divine_txt = divine_txt.replace(\"\\n\",\" \")\n",
        "divine_txt = divine_txt.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MOxj8ig_xCg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso siamo pronti per tokenizzare il testo, usiamo spacy per farlo. Se non lo abbiamo già fatto installiamo il modulo per la lingua italiana."
      ]
    },
    {
      "metadata": {
        "id": "1p0leKS7ed-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "4e36b189-0a67-4c77-bb97-d3512eacadc1"
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download it_core_news_sm"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting it_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.0.0/it_core_news_sm-2.0.0.tar.gz#egg=it_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.0.0/it_core_news_sm-2.0.0.tar.gz (36.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 36.5MB 91.3MB/s \n",
            "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
            "  Running setup.py install for it-core-news-sm ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed it-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/it_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/it_core_news_sm\n",
            "\n",
            "    You can now load the model via spacy.load('it_core_news_sm')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IDWfVjyzAECM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Carichiamolo, definiamo una funzione che estrae i token da tutto il testo ed utilizziamola."
      ]
    },
    {
      "metadata": {
        "id": "Uw_ynE1men3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "03b19500-4e86-4bf6-f668-4d0a6798a8e2"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "  tokens = nlp(text)\n",
        "  tokens_filtered = [token.text for token in tokens]\n",
        "  return tokens_filtered\n",
        "\n",
        "tokens = preprocess(divine_txt)\n",
        "tokens[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nel',\n",
              " 'mezzo',\n",
              " 'del',\n",
              " 'cammin',\n",
              " 'di',\n",
              " 'nostra',\n",
              " 'vita',\n",
              " 'mi',\n",
              " 'ritrovai',\n",
              " 'per']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "CAyfyrqeCacv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Suddividiamo il testo in sequenze con una lunghezza massima di 10 parole, lo scopo delle nostra rete sarà quello di predire l'ultima parola della sequenza utilizzando quelle precedenti."
      ]
    },
    {
      "metadata": {
        "id": "6oXESRj8dUx3",
        "colab_type": "code",
        "outputId": "1bb8f9bd-9d6b-402c-9936-30143550aa96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "maxlen = 10\n",
        "\n",
        "divine_sents = []\n",
        "\n",
        "for i in range(maxlen, len(tokens)):\n",
        "  divine_sents.append(tokens[i-maxlen:i])\n",
        "  \n",
        "print(divine_sents[0])\n",
        "print(divine_sents[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['nel', 'mezzo', 'del', 'cammin', 'di', 'nostra', 'vita', 'mi', 'ritrovai', 'per']\n",
            "['mezzo', 'del', 'cammin', 'di', 'nostra', 'vita', 'mi', 'ritrovai', 'per', 'una']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AoHbrCD6APDb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso dobbiamo codificare le parole in numeri, possiamo farlo creandoci un dizionario di tutte le parole contenute nel testo e poi sostituire i token di ogni frase con la corrispondente posizione della parola nel dizionario. Per farlo possiamo usare direttamente la classe *Tokenizer* di keras che fa tutto per noi."
      ]
    },
    {
      "metadata": {
        "id": "ZLBDHWQZWpm8",
        "colab_type": "code",
        "outputId": "b79f6040-0190-4b04-f67d-f6acde6546a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(divine_sents)\n",
        "divine_sents = tokenizer.texts_to_sequences(divine_sents)\n",
        "\n",
        "divine_sents[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[38, 224, 24, 603, 4, 186, 153, 15, 13574, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "VsX1IDh4DInb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creiamo i set con features e target, come già detto le features saranno i tokens di una sequenza eccetto l'ultimo, il target sarà invece proprio quest'ultimo token."
      ]
    },
    {
      "metadata": {
        "id": "24kyU6HsiTf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "divine_sents = np.array(divine_sents)\n",
        "\n",
        "X = divine_sents[:,:-1]\n",
        "y = divine_sents[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ce3zIOi0DRa7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il nostro si tratta di un problema di classificazione multiclasse, le cui possibili classi sono tutte le parole contenute nel dizionario, vediamo quante sono esattamente."
      ]
    },
    {
      "metadata": {
        "id": "jrMSeOUxDbi_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c861c16-327b-411d-9bee-6f57c0efa4ac"
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_counts)\n",
        "vocab_size"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13574"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "C42AzDVdDhl-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Abbiamo in totale 13575 parole, usiamo la funzione *to_categorical(y)* di keras per eseguire il one hot encoding delle variabili target."
      ]
    },
    {
      "metadata": {
        "id": "UteQnvyqcXbl",
        "colab_type": "code",
        "outputId": "81c73c85-5df8-4b14-8f2d-4b11d8ba8bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size+1)\n",
        "y.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97393, 13575)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "SVLejwLXDtto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creazione della Rete Ricorrente\n",
        "Creiamo la nostra architettura di rete neurale ricorrente:\n",
        " - Utilizziamo il *Word Embedding* per creare una rappresentazione vettoriale delle parole, addestrandolo sul nostro corpus di testo.\n",
        " - Aggiungiamo due strati ricorrenti con 50 nodi ciascuno, il primo dei quali dovrà ritornare una sequenza che servirà come input per il secondo.\n",
        " - Aggiungiamo un terzo strato denso con sempre 50 nodi.\n",
        " - Infine inseriamo uno strato di output con un numero di nodi ovviamente pari al numero di parole nel dizionario."
      ]
    },
    {
      "metadata": {
        "id": "zunahaBbkbdc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import Model, Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size+1, maxlen-1, input_length=maxlen-1))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(50, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size+1, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XbIju0mJE6tI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello, trattandosi di un problema di classificazione multiclasse useremo la *categorical crossentropy* come funzione di costo."
      ]
    },
    {
      "metadata": {
        "id": "7Y29LJ9Dk596",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIwa1NW2FNTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come abbiamo detto, quello che vogliamo fare è creare una rete neurale in grado di generare testo dantesco, quindi perché stiamo eseguendo una classificazione ? L'utilizzo che faremo della rete è il seguente:\n",
        "1. Forniremo alla rete del testo 'seed' di una lunghezza prestabilita, cioè del testo di base che poi essa userà per generare quello seguente, possiamo definire noi tale testo oppure estrarlo a caso dal corpus.\n",
        "2. La rete predirrà la parola che secondo essa dovrebbe seguire il testo 'seed'.\n",
        "3. Aggiungiamo la parola predetta al testo.\n",
        "4. Rimuoviamo la prima parola del testo.\n",
        "5. Ripetiamo i punti da 2 a 4 fino a quando il testo predetto non avrà la lunghezza che vogliamo.\n",
        "\n",
        "Definiamo una funzione che fa esattamente questo."
      ]
    },
    {
      "metadata": {
        "id": "OyIROaQBywkt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate(seed=None, rand_seed_len=10, generate_len=25):\n",
        "  \n",
        "  output = \"\"\n",
        "  \n",
        "  if(seed==None):\n",
        "    start_index = randint(0, len(divine_txt))\n",
        "    text = divine_txt[start_index:start_index+rand_seed_len]\n",
        "  else:\n",
        "    text = re.sub(r'[^\\w\\s]','', seed.lower())\n",
        "    \n",
        "  for i in range(generate_len):\n",
        "    tokens = np.array(tokenizer.texts_to_sequences([text]))\n",
        "    tokens = pad_sequences(tokens, maxlen=maxlen-1, truncating=\"pre\")\n",
        "      \n",
        "    pred_word = model.predict_classes([tokens])[0]\n",
        "    pred_word = tokenizer.index_word[pred_word]\n",
        "      \n",
        "    text+=\" \"+pred_word\n",
        "    output+=pred_word+\" \"\n",
        "    \n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dmRL1QNLGdIA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utilizzando keras è possibile definire una funzione che viene eseguita al termine di ogni epoca dell'addestramento, definiamo una funzione che chiama la funzione per generare il testo, in modo tale da vedere come la qualità del testo varia durante l'addestramento."
      ]
    },
    {
      "metadata": {
        "id": "EZ_xC_G4GdoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_on_epoch(epoch, _):\n",
        "  output = generate()\n",
        "  print('Dante dice: \"'+output+'\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-3bwHvkGwpU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso siamo pronti per l'addestramento, per chiamare la funzione appena definita ad ogni epoca dobbiamo utilizzare i callback.\n",
        "Creiamo un Lambda Callback passando all'interno del parametro on_epoch_end il nome della funzione.\n",
        "Aggiungiamo il callback all'interno del parametro *callbacks* del metodo *.fit()*.\n",
        "<br>\n",
        "Keras ci mette a disposizone diversi callbacks da utilizzare durante l'addestramento, un'altro molto utile è quello per eseguire **l'early stopping**, cioè quella tecnica che ci permette di terminare l'addestramento in anticipo se la qualità del modello non sta migliorando. Utilizziamo l'early stopping con la classe *EarlyStopping* utilizzando i parametri *min_delta* e *patience* per interrompere l'addestramento se il valore della log loss non migliora di almeno 0.001 dopo 5 epoche.\n",
        "<br><br>\n",
        "**NOTA BENE**\n",
        "<br>\n",
        "Se non hai una GPU che supporta la tecnologia CUDA e non vuoi usare Google Colaboratory, ti consiglio di importare il modello pre-addestrato eseguendo il codice nella cella poco più in basso, altrimenti l'addestramento potrebbe richiedere anche giorni e mettere sotto forte stress il tuo pc."
      ]
    },
    {
      "metadata": {
        "id": "TEjQFmPklCQ4",
        "colab_type": "code",
        "outputId": "d4135d8d-93dd-4f60-8fa7-b206ce939932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5231
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, LambdaCallback\n",
        "\n",
        "epoch_end_callback = LambdaCallback(on_epoch_end=generate_on_epoch)\n",
        "earlyStopping = EarlyStopping(min_delta=0.001, patience=5)\n",
        "model.fit(X, y, batch_size=128, epochs=500, callbacks=[earlyStopping, epoch_end_callback])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "97393/97393 [==============================] - 51s 521us/step - loss: 7.4202 - acc: 0.0398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dante dice: \"e e e e e e e e e e e e e e e e e e e e e e e e e \"\n",
            "Epoch 2/100\n",
            "97393/97393 [==============================] - 47s 487us/step - loss: 7.0664 - acc: 0.0413\n",
            "Dante dice: \"e e e e e e e e e e e e e e e e e e e e e e e e e \"\n",
            "Epoch 3/100\n",
            "97393/97393 [==============================] - 47s 478us/step - loss: 6.9641 - acc: 0.0439\n",
            "Dante dice: \"che che che che che che che che che che che che che che che che che che che che che che che che che \"\n",
            "Epoch 4/100\n",
            "97393/97393 [==============================] - 47s 480us/step - loss: 6.8629 - acc: 0.0526\n",
            "Dante dice: \"la la la la la la la la la la la la la la la la la la la la la la la la la \"\n",
            "Epoch 5/100\n",
            "97393/97393 [==============================] - 47s 479us/step - loss: 6.7695 - acc: 0.0566\n",
            "Dante dice: \"la occhi e che che che che che che che che che che che che che che che che che che che che che che \"\n",
            "Epoch 6/100\n",
            "97393/97393 [==============================] - 46s 477us/step - loss: 6.6758 - acc: 0.0594\n",
            "Dante dice: \"che la occhi e la occhi e la occhi e la occhi e la occhi e la occhi e la occhi e la occhi e \"\n",
            "Epoch 7/100\n",
            "97393/97393 [==============================] - 46s 476us/step - loss: 6.5894 - acc: 0.0611\n",
            "Dante dice: \"l occhi e che che che che che che che che che che che che che che che che che che che che che che \"\n",
            "Epoch 8/100\n",
            "97393/97393 [==============================] - 46s 475us/step - loss: 6.5108 - acc: 0.0629\n",
            "Dante dice: \"e l occhi e l occhi e l occhi e che la occhi e che la occhi e che la occhi e che la occhi \"\n",
            "Epoch 9/100\n",
            "97393/97393 [==============================] - 47s 480us/step - loss: 6.4443 - acc: 0.0649\n",
            "Dante dice: \"l occhi e si occhi e si occhi e si occhi e si occhi e si occhi e si occhi e si occhi e si \"\n",
            "Epoch 10/100\n",
            "97393/97393 [==============================] - 47s 482us/step - loss: 6.3819 - acc: 0.0667\n",
            "Dante dice: \"si occhi e si occhi che l occhi che si occhi e si occhi e si suo vita e si suo vita e l occhi \"\n",
            "Epoch 11/100\n",
            "97393/97393 [==============================] - 48s 489us/step - loss: 6.3173 - acc: 0.0686\n",
            "Dante dice: \"che l occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che \"\n",
            "Epoch 12/100\n",
            "97393/97393 [==============================] - 46s 476us/step - loss: 6.2479 - acc: 0.0709\n",
            "Dante dice: \"che l occhi che si sua vita che non si occhi che non si occhi e si sua vita che non si sua vita che \"\n",
            "Epoch 13/100\n",
            "97393/97393 [==============================] - 46s 475us/step - loss: 6.1734 - acc: 0.0730\n",
            "Dante dice: \"che l occhi che l occhi che l occhi che non si occhi che non si occhi che l occhi che non si occhi che \"\n",
            "Epoch 14/100\n",
            "97393/97393 [==============================] - 46s 474us/step - loss: 6.0971 - acc: 0.0750\n",
            "Dante dice: \"non si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che \"\n",
            "Epoch 15/100\n",
            "97393/97393 [==============================] - 47s 478us/step - loss: 6.0140 - acc: 0.0780\n",
            "Dante dice: \"che l suo maestro che non si sue si sue si sue si sue si sue si sue si sue si sue si sue si \"\n",
            "Epoch 16/100\n",
            "97393/97393 [==============================] - 47s 487us/step - loss: 5.9327 - acc: 0.0803\n",
            "Dante dice: \"sua vita che si sue vita che si occhi che si sue si sue si sue si sue si sue si sue si sue si \"\n",
            "Epoch 17/100\n",
            "97393/97393 [==============================] - 46s 476us/step - loss: 5.8525 - acc: 0.0836\n",
            "Dante dice: \"che si occhi che si occhi e che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi \"\n",
            "Epoch 18/100\n",
            "97393/97393 [==============================] - 46s 473us/step - loss: 5.7759 - acc: 0.0865\n",
            "Dante dice: \"si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si \"\n",
            "Epoch 19/100\n",
            "97393/97393 [==============================] - 46s 469us/step - loss: 5.7025 - acc: 0.0895\n",
            "Dante dice: \"sua vita che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si \"\n",
            "Epoch 20/100\n",
            "97393/97393 [==============================] - 46s 470us/step - loss: 5.6317 - acc: 0.0922\n",
            "Dante dice: \"la suo donna che l suo duca che si occhi e si occhi che si occhi che si occhi che si occhi che si occhi \"\n",
            "Epoch 21/100\n",
            "97393/97393 [==============================] - 46s 469us/step - loss: 5.5663 - acc: 0.0943\n",
            "Dante dice: \"si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si occhi che si \"\n",
            "Epoch 22/100\n",
            "97393/97393 [==============================] - 46s 472us/step - loss: 5.5017 - acc: 0.0955\n",
            "Dante dice: \"e si sue parole ministri e che tu si sue note e l suo duca e si sue parole si cose che si sue parole \"\n",
            "Epoch 23/100\n",
            "97393/97393 [==============================] - 46s 472us/step - loss: 5.4385 - acc: 0.0979\n",
            "Dante dice: \"che si sue note e l mondo e si fece che si sue parole e si sue parole si occhi che si sue parole si \"\n",
            "Epoch 24/100\n",
            "97393/97393 [==============================] - 46s 470us/step - loss: 5.3778 - acc: 0.0992\n",
            "Dante dice: \"e si sue arti e si sue orme che l mondo e si sue orme che l mondo miei si cose che si disse che \"\n",
            "Epoch 25/100\n",
            "97393/97393 [==============================] - 46s 471us/step - loss: 5.3186 - acc: 0.1010\n",
            "Dante dice: \"che si sue note e si sue orme che l suo duca e si fece e l suo duca e si genti che si genti \"\n",
            "Epoch 26/100\n",
            "97393/97393 [==============================] - 46s 468us/step - loss: 5.2631 - acc: 0.1026\n",
            "Dante dice: \"e si sue arti e non puote e si sue vita che l ciel e si sue orme che l ciel e si sue orme \"\n",
            "Epoch 27/100\n",
            "97393/97393 [==============================] - 46s 468us/step - loss: 5.2113 - acc: 0.1034\n",
            "Dante dice: \"mondo sensibile e fecero a la sua donna che si sue vita che si genti che si genti disparmente si genti onde non si genti \"\n",
            "Epoch 28/100\n",
            "97393/97393 [==============================] - 45s 466us/step - loss: 5.1620 - acc: 0.1059\n",
            "Dante dice: \"femi quelle sanesi innata si mosse e l mondo che si sue ali a muoverci e io e si sue arti sedesse parea che tu \"\n",
            "Epoch 29/100\n",
            "97393/97393 [==============================] - 45s 467us/step - loss: 5.1173 - acc: 0.1086\n",
            "Dante dice: \"almi e si partì e non starà taccia si genti disparmente si genti macolato e si fa e l suo maestro e l mondo e \"\n",
            "Epoch 30/100\n",
            "97393/97393 [==============================] - 46s 467us/step - loss: 5.0763 - acc: 0.1108\n",
            "Dante dice: \"che sono discendesse e che corone parte ond io altresì dove matrimonio e l quale mio si genti e si fu sue si parole e \"\n",
            "Epoch 31/100\n",
            "97393/97393 [==============================] - 46s 467us/step - loss: 5.0362 - acc: 0.1126\n",
            "Dante dice: \"secoli aiutino e la tua donna che femi indugio e non intrava e l suo maestro e l mondo e non si parole e non \"\n",
            "Epoch 32/100\n",
            "97393/97393 [==============================] - 46s 469us/step - loss: 5.0031 - acc: 0.1146\n",
            "Dante dice: \"e tarde e io riprendo e tu e si genti schiarar quelle rivesta io vidi non ti percotean non altrimenti e ciascheduno e tu a \"\n",
            "Epoch 33/100\n",
            "97393/97393 [==============================] - 46s 468us/step - loss: 4.9643 - acc: 0.1176\n",
            "Dante dice: \"lorbita che tu e si genti disparmente si genti e si partì di mente e l mondo e si cose sfavillaro le sue parole cominciando \"\n",
            "Epoch 34/100\n",
            "97393/97393 [==============================] - 46s 469us/step - loss: 4.9333 - acc: 0.1194\n",
            "Dante dice: \"e calisto e pennuto lacciuoli e pompeo e corniglia e la sua si braccia e si rinselva gonfia e l suo fattor che l maestro \"\n",
            "Epoch 35/100\n",
            "97393/97393 [==============================] - 45s 467us/step - loss: 4.8999 - acc: 0.1214\n",
            "Dante dice: \"ritenere dammirazione                                                                                                                                                                  \"\n",
            "Epoch 36/100\n",
            "97393/97393 [==============================] - 45s 462us/step - loss: 4.8696 - acc: 0.1225\n",
            "Dante dice: \"qua permanendo a verona e la sua fiala per la sua vita estatica la mia donna che non può si cose onde l mondo e \"\n",
            "Epoch 37/100\n",
            "97393/97393 [==============================] - 45s 461us/step - loss: 4.8395 - acc: 0.1254\n",
            "Dante dice: \"e mestiere agnello ond è mestiere emisperi e lanimo che l suo viso che cortesia larcano per esso e tu e tu e si genti \"\n",
            "Epoch 38/100\n",
            "97393/97393 [==============================] - 45s 458us/step - loss: 4.8068 - acc: 0.1289\n",
            "Dante dice: \"gridaro a me che tu si posano a sua ritenere onde si partì di sua vita incognita e perché linfimo clugnì di la sua mente \"\n",
            "Epoch 39/100\n",
            "97393/97393 [==============================] - 44s 457us/step - loss: 4.7842 - acc: 0.1288\n",
            "Dante dice: \"lunghesso qua sobria e cusce per la puntura e l suo fulgore e vidi restarmi                                                                       \"\n",
            "Epoch 40/100\n",
            "97393/97393 [==============================] - 44s 455us/step - loss: 4.7513 - acc: 0.1313\n",
            "Dante dice: \"della le sue parole immense fie ed elli a drittura e più avvantaggio che l suo lizio e come vantaggio e l ciel che si \"\n",
            "Epoch 41/100\n",
            "97393/97393 [==============================] - 44s 454us/step - loss: 4.7245 - acc: 0.1339\n",
            "Dante dice: \"le sue avello iperïone e turchi casti che passarmen scriva e consonanti e l mondo schietto e lascoltar l occhi e tu e ricolte limage \"\n",
            "Epoch 42/100\n",
            "97393/97393 [==============================] - 44s 448us/step - loss: 4.6937 - acc: 0.1366\n",
            "Dante dice: \"servammo e mossa conviensi e batti e sua oppilazion che fuor e tamò pranse sortiro dove tu borea per la strozza impeli uomo fisi e \"\n",
            "Epoch 43/100\n",
            "97393/97393 [==============================] - 43s 441us/step - loss: 4.6712 - acc: 0.1381\n",
            "Dante dice: \"giro dilatate falde che colsi ad avvisar e ieptè a la sua vista e l mondo e non fïate e si genti onde cavalchi e \"\n",
            "Epoch 44/100\n",
            "97393/97393 [==============================] - 43s 440us/step - loss: 4.6441 - acc: 0.1401\n",
            "Dante dice: \"disiderate e di fegghine berzaglio e la mia lizio e dabitanti gualandi lo primiero profani ond elli polveroso perdea che pargoletta in su la sua \"\n",
            "Epoch 45/100\n",
            "97393/97393 [==============================] - 43s 443us/step - loss: 4.6184 - acc: 0.1439\n",
            "Dante dice: \"pettinaio e antomata e indistinto scorgessi di sua vista e l suo duca e vidi non è coruscar di poco giro che l suo duca \"\n",
            "Epoch 46/100\n",
            "97393/97393 [==============================] - 42s 436us/step - loss: 4.5936 - acc: 0.1448\n",
            "Dante dice: \"suo verbo e discarcate le sue ali che savvalorava e per secoli veggendoci che tu venis e non fa ritornarci l ciel e non si \"\n",
            "Epoch 47/100\n",
            "97393/97393 [==============================] - 42s 432us/step - loss: 4.5707 - acc: 0.1482\n",
            "Dante dice: \"vidine ed elli a me custodi sì che l senso cammin ripiglierà temps però ermafrodito e orbisaglia che disvele o niso per la sua via \"\n",
            "Epoch 48/100\n",
            "97393/97393 [==============================] - 42s 428us/step - loss: 4.5461 - acc: 0.1490\n",
            "Dante dice: \"denti taperse usciteci salsi le sue parole e non si fu danca sì che parton tu che l maestro mio si fu che la lingua \"\n",
            "Epoch 49/100\n",
            "97393/97393 [==============================] - 42s 430us/step - loss: 4.5215 - acc: 0.1527\n",
            "Dante dice: \"magino e l suo furor arrigo ogn le scalee la navicanti di sua città combatte e vau caino e la mazza chiesa ond elli appare \"\n",
            "Epoch 50/100\n",
            "97393/97393 [==============================] - 42s 429us/step - loss: 4.5000 - acc: 0.1543\n",
            "Dante dice: \"secoli volesti e lance le parole sue sapem di fame e liber a guisa la terra che riguardolla ammiri di sua fretta così vid io \"\n",
            "Epoch 51/100\n",
            "97393/97393 [==============================] - 42s 430us/step - loss: 4.4747 - acc: 0.1572\n",
            "Dante dice: \"anciso vergognando mia balba non non contingenze attivi o vau chuscisser di sangue e in qua cred a recar parti non si disuna e la \"\n",
            "Epoch 52/100\n",
            "97393/97393 [==============================] - 42s 430us/step - loss: 4.4576 - acc: 0.1585\n",
            "Dante dice: \"denti ecce passammo anastasio romor obietto ond elli a piglio sentire avvolte e l punir e non è si cose che fregiavan sì che l \"\n",
            "Epoch 53/100\n",
            "97393/97393 [==============================] - 42s 428us/step - loss: 4.4316 - acc: 0.1616\n",
            "Dante dice: \"bologna apprende ma non dispiega e scendean si sue donna che netta la tre quesper cornute e sammenta di terra portar lerta alluma fossero pacifici \"\n",
            "Epoch 54/100\n",
            "97393/97393 [==============================] - 42s 436us/step - loss: 4.4141 - acc: 0.1635\n",
            "Dante dice: \"scorno che la lingua che barbare mill corrëan occhi che presaga per le fessure e l sabbion schietto ma campo e disvele collo e trane \"\n",
            "Epoch 55/100\n",
            "97393/97393 [==============================] - 44s 448us/step - loss: 4.3918 - acc: 0.1651\n",
            "Dante dice: \"opizzo di verona di casa che ferute fossero di ridure io mi disse di cui nata e poscia jausen la lingua etterna permesso scuote e \"\n",
            "Epoch 56/100\n",
            "97393/97393 [==============================] - 44s 452us/step - loss: 4.3717 - acc: 0.1679\n",
            "Dante dice: \"infernal e perugia uscivan quelle scovrire in lamoroso qual si disfanno non parlai omberto e tante può redire non pianura ed ènne a me pazzo \"\n",
            "Epoch 57/100\n",
            "97393/97393 [==============================] - 45s 457us/step - loss: 4.3581 - acc: 0.1685\n",
            "Dante dice: \"povertà del mondo torca del mondo torca e ignota est che minaccia avvisando denti linfiammata cortesia di sua moltitudine volante gioseppo specifica dov a me \"\n",
            "Epoch 58/100\n",
            "97393/97393 [==============================] - 44s 457us/step - loss: 4.3358 - acc: 0.1708\n",
            "Dante dice: \"per lorizzonta e l suo don che quai l ciel prescritto le sue parole a la sua bontate che l sol declina e anima armati \"\n",
            "Epoch 59/100\n",
            "97393/97393 [==============================] - 44s 452us/step - loss: 4.3146 - acc: 0.1740\n",
            "Dante dice: \"giro maspettava e la gorgona e chinati e mariti sudìe date che se non si fu cantaron per la sua mente che non ritrova e \"\n",
            "Epoch 60/100\n",
            "97393/97393 [==============================] - 45s 457us/step - loss: 4.3027 - acc: 0.1750\n",
            "Dante dice: \"che tolto e poi che la mia mente e l ciel e si può che più e si può che ncise le sue parole e \"\n",
            "Epoch 61/100\n",
            "97393/97393 [==============================] - 46s 470us/step - loss: 4.2815 - acc: 0.1777\n",
            "Dante dice: \"fiorentin perdono e farsalia pigli non altrimenti e ciascun non non discernessi io stava ond io vidi l mondo e soggiugnendo e mai non si \"\n",
            "Epoch 62/100\n",
            "97393/97393 [==============================] - 46s 468us/step - loss: 4.2612 - acc: 0.1804\n",
            "Dante dice: \"iv io cera che lassonnar per misurar la divina bontà e in parisi soglie immense chesce la zucca de la bocca parolette si è non \"\n",
            "Epoch 63/100\n",
            "97393/97393 [==============================] - 46s 469us/step - loss: 4.2455 - acc: 0.1816\n",
            "Dante dice: \"portan lattender innumerabili sua facultate e discarcate le fronde brune che anfisibena per la sua animo volante campagna scaleo toglieva io che l maestro e \"\n",
            "Epoch 64/100\n",
            "97393/97393 [==============================] - 46s 469us/step - loss: 4.2310 - acc: 0.1835\n",
            "Dante dice: \"bona vago e parleremo là salsi me che tu tinfiamma e salvatico saccompagna l batista                                                                       \"\n",
            "Epoch 65/100\n",
            "97393/97393 [==============================] - 46s 472us/step - loss: 4.2170 - acc: 0.1855\n",
            "Dante dice: \"beni sospeccioso e batti vedrami io che non si genti taccia bonagiunta testimon che mero e bruggia imponne e mirra che lamore né sediero per \"\n",
            "Epoch 66/100\n",
            "97393/97393 [==============================] - 46s 477us/step - loss: 4.2017 - acc: 0.1859\n",
            "Dante dice: \"pelle cangia pover amècche cosa agro e poi lestrema e diserrando quante toccata vegghia profeta e io moronto intero dal cuoio e prendemmo la qual \"\n",
            "Epoch 67/100\n",
            "97393/97393 [==============================] - 46s 469us/step - loss: 4.1848 - acc: 0.1893\n",
            "Dante dice: \"denti ecce passammo anastasio romor schietto e scendean non trasmuta accetto ma si racqueta minerva che torni a parlamento di retro o innumerabili iattanza avvolti \"\n",
            "Epoch 68/100\n",
            "97393/97393 [==============================] - 45s 466us/step - loss: 4.1670 - acc: 0.1914\n",
            "Dante dice: \"punzelli ond è mestiere usciro poscia tassenno e di punto e ponticelli o soverchia e li occhi e soggioga la sua fiumicello che non fossi \"\n",
            "Epoch 69/100\n",
            "97393/97393 [==============================] - 45s 465us/step - loss: 4.1478 - acc: 0.1933\n",
            "Dante dice: \"malluminasti antico poi bevete di le nostre spalle e ronca di gratüito dispaia sì che l mondo ovile saggiri venirli e frate aglauro che superbite \"\n",
            "Epoch 70/100\n",
            "97393/97393 [==============================] - 46s 469us/step - loss: 4.1418 - acc: 0.1950\n",
            "Dante dice: \"discorde e farsi posta sentiranno e orbisaglia e notabili trasmuta israèl sì che si fa e in su la sua città che muor per me \"\n",
            "Epoch 71/100\n",
            "97393/97393 [==============================] - 45s 465us/step - loss: 4.1230 - acc: 0.1961\n",
            "Dante dice: \"chandavamo e flegetonta e meleagro e antifonte simonide agatone sì che l fratel de buccolici la ubi e con lagute sue sue refugio e la \"\n",
            "Epoch 72/100\n",
            "97393/97393 [==============================] - 45s 463us/step - loss: 4.1116 - acc: 0.1978\n",
            "Dante dice: \"vidili sua dimostrazion quanto fermi inteso soli e la sai e non è laffezione di lor vivagni e che tu che si digrada che passammo \"\n",
            "Epoch 73/100\n",
            "97393/97393 [==============================] - 45s 464us/step - loss: 4.0977 - acc: 0.1993\n",
            "Dante dice: \"additandomi io divenni ve le sue meschite e pensava più a lui procedere ogne spada e si distende che la lingua incontra mi disse che \"\n",
            "Epoch 74/100\n",
            "97393/97393 [==============================] - 45s 462us/step - loss: 4.0788 - acc: 0.2024\n",
            "Dante dice: \"veduti giuso ve surga andai giù negligenza saria parea pagando per che di lui suggelli più e giovato che ripriego e dimmi non e false \"\n",
            "Epoch 75/100\n",
            "97393/97393 [==============================] - 45s 465us/step - loss: 4.0691 - acc: 0.2031\n",
            "Dante dice: \"dicean previsa grifagno ad esse e legato e quella suono splendori dovieti qual podesta mbedue e quel da lui avrian la sua dota rimossi di \"\n",
            "Epoch 76/100\n",
            "97393/97393 [==============================] - 46s 468us/step - loss: 4.0576 - acc: 0.2047\n",
            "Dante dice: \"lampiezza né darei che parleremo ond anibàl a la marina ingrossò vedete falsificando viniziani artista quando non che tu oderisi ello saremmo a riconoscer al \"\n",
            "Epoch 77/100\n",
            "97393/97393 [==============================] - 46s 468us/step - loss: 4.0467 - acc: 0.2074\n",
            "Dante dice: \"esto lievre che innumerabili insegna e non che non avessi adempie che faree sù che l duca mio incontr o cherco mofferse che in parisi \"\n",
            "Epoch 78/100\n",
            "97393/97393 [==============================] - 45s 465us/step - loss: 4.0336 - acc: 0.2073\n",
            "Dante dice: \"aspre rimasero e azzurro che poi mea parlato e lavessi usava le parole che l maestro mio in su la favola disopo che di là \"\n",
            "Epoch 79/100\n",
            "97393/97393 [==============================] - 45s 463us/step - loss: 4.0175 - acc: 0.2107\n",
            "Dante dice: \"sigillo che buondelmonte e lessemplare di scandalo e laltra masnada a che modo sarieno a riveder l scoglio che sorda e dimmi lanima biado io \"\n",
            "Epoch 80/100\n",
            "97393/97393 [==============================] - 46s 472us/step - loss: 4.0071 - acc: 0.2116\n",
            "Dante dice: \"messi e per iscede aspetti che l segno che scaccia e laltra offerta che la mia vista e l maestro e quando si creda e \"\n",
            "Epoch 81/100\n",
            "97393/97393 [==============================] - 46s 471us/step - loss: 4.0007 - acc: 0.2109\n",
            "Dante dice: \"lista silvestro e l tuo maestro che l mondo e si cose sue si puote e più recirculando lucemi e messe spia lassi ma recalcitrate \"\n",
            "Epoch 82/100\n",
            "97393/97393 [==============================] - 47s 485us/step - loss: 3.9839 - acc: 0.2145\n",
            "Dante dice: \"corta surgere e tigri foco superno avendomi tosto e di esto lievre pur scoverto più e in te che agevolemente fero stava e poi in \"\n",
            "Epoch 83/100\n",
            "97393/97393 [==============================] - 47s 483us/step - loss: 3.9713 - acc: 0.2169\n",
            "Dante dice: \"purga quando si lamenta leggero ditalia lassentir la sua rosa quando poscia de forlì per non angeli soffi di caprona oreste rimontò e ser pegasëa \"\n",
            "Epoch 84/100\n",
            "97393/97393 [==============================] - 47s 484us/step - loss: 3.9621 - acc: 0.2171\n",
            "Dante dice: \"digrada che miglia ma dimmi non corra e ignota regi antiche che l mondo sacro bestemmiavano nviluppata pensare bontate incominciò tu che da circüir le \"\n",
            "Epoch 85/100\n",
            "97393/97393 [==============================] - 47s 483us/step - loss: 3.9496 - acc: 0.2190\n",
            "Dante dice: \"sicure oltraggio frequente a tizio lo mondo errante che l maestro a la sua sorte e penètra chusai e percosselo ad esse sù od e \"\n",
            "Epoch 86/100\n",
            "97393/97393 [==============================] - 47s 483us/step - loss: 3.9375 - acc: 0.2201\n",
            "Dante dice: \"o atti avien se l mondo star gent io vidi or la donna e riguardolla lava che tante mossero e tebaldello movemmo ma se sani \"\n",
            "Epoch 87/100\n",
            "97393/97393 [==============================] - 47s 484us/step - loss: 3.9284 - acc: 0.2207\n",
            "Dante dice: \"rïempie che risplendea lanche e io è posposta di terra e l mondo mi disnodi e si richiudon che la voce sabbandoni del cappuccio e \"\n",
            "Epoch 88/100\n",
            "97393/97393 [==============================] - 47s 481us/step - loss: 3.9084 - acc: 0.2242\n",
            "Dante dice: \"falterona e dosso e con istrane sì che li altri volgere e arpa e corniglia io vidi sonar né vau genti doro e caddi che \"\n",
            "Epoch 89/100\n",
            "97393/97393 [==============================] - 47s 483us/step - loss: 3.9040 - acc: 0.2246\n",
            "Dante dice: \"sicure ammenda soli degno ma si purga ma l mondo e sappone e ardinghi e diserrare parvemi che disvele e libente a riveder la ceppo \"\n",
            "Epoch 90/100\n",
            "97393/97393 [==============================] - 47s 485us/step - loss: 3.8919 - acc: 0.2269\n",
            "Dante dice: \"regi sta conosce procedendo simpingua di beda e di marcel alleluia ma le sprona che                     \"\n",
            "Epoch 91/100\n",
            "97393/97393 [==============================] - 47s 485us/step - loss: 3.8822 - acc: 0.2277\n",
            "Dante dice: \"specchiai e drizzo di ridure ché io che tu falsasti che si vostre opere pote la sua e l sasso che si monta guizzando si \"\n",
            "Epoch 92/100\n",
            "97393/97393 [==============================] - 47s 484us/step - loss: 3.8696 - acc: 0.2291\n",
            "Dante dice: \"vestigio chunanima soli e tai si tace e orazïone a me che tu tinfiamma ma sargomenta non spiritu marche se la sua sentenza e maggi \"\n",
            "Epoch 93/100\n",
            "97393/97393 [==============================] - 47s 480us/step - loss: 3.8655 - acc: 0.2293\n",
            "Dante dice: \"spreme che perugia ragionato chudito potert io trasmutare e bee per lo nferno scuri di santa a la sua grazia che additò a me che \"\n",
            "Epoch 94/100\n",
            "97393/97393 [==============================] - 47s 479us/step - loss: 3.8529 - acc: 0.2325\n",
            "Dante dice: \"daguglion per me insieme sadiri la selva mio si genti sue sorda e quindi legava giù ritorci li occhi ond al viso e papi e \"\n",
            "Epoch 95/100\n",
            "97393/97393 [==============================] - 47s 485us/step - loss: 3.8388 - acc: 0.2338\n",
            "Dante dice: \"falca per dio che tu che tu e spenti ma per la pineta al sole e l duca venuto conosce liquefatta tornand io mi disse \"\n",
            "Epoch 96/100\n",
            "97393/97393 [==============================] - 47s 486us/step - loss: 3.8287 - acc: 0.2345\n",
            "Dante dice: \"per sangue vuolsi pasife di retro e se tu dottobre siate tacere a la sua madre guadi a chiappa a la sua nutrice volante unquam \"\n",
            "Epoch 97/100\n",
            "97393/97393 [==============================] - 47s 485us/step - loss: 3.8208 - acc: 0.2365\n",
            "Dante dice: \"a cieldauro ad piedi e io da dubitar chassolver sette non e tu che si sigillava e prendemmo la gloriosa tormento che nel nferno portinaio \"\n",
            "Epoch 98/100\n",
            "97393/97393 [==============================] - 47s 485us/step - loss: 3.8072 - acc: 0.2384\n",
            "Dante dice: \"monferrato e vidili galigaio dorata in mar di conversione dimostrami e con autoritadi e io notai non renduto a essa e son megera dispregiare a \"\n",
            "Epoch 99/100\n",
            "97393/97393 [==============================] - 47s 483us/step - loss: 3.7952 - acc: 0.2388\n",
            "Dante dice: \"solfo spessa curan che quel che non non l scrivo ma l mondo e si diparte limago e di mirar tutte tutte noi nimica e \"\n",
            "Epoch 100/100\n",
            "97393/97393 [==============================] - 47s 482us/step - loss: 3.7871 - acc: 0.2414\n",
            "Dante dice: \"alluma vituperio la saette di dio che l pregato che più precinto di lei che la pelle io ti solve travi per la sua mente \"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f68b9e40ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "CVgScJ9NIvs3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se stai usando una GPU che supporta la tecnologia CUDA, sul tuo computer o con Google Colaboratory, l'addestramento per 500 epoche dovrebbe richiedere un paio di ore, se non vuoi aspettare puoi ridurre il numero di epoche a non meno di 100 oppure importare il modello che ho già addestrato eseguendo il codice qui sotto."
      ]
    },
    {
      "metadata": {
        "id": "U42_UVaeJPzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "from keras.models import load_model\n",
        "\n",
        "model_file = \"dante_500.h5\"\n",
        "model_path = \"https://github.com/ProfAI/nlp00/raw/master/9%20-%20Reti%20Ricorrenti%20e%20Text%20Generation/model/dante_500.h5\"\n",
        "\n",
        "urlretrieve(model_path, model_file)\n",
        "\n",
        "model = load_model(model_file)\n",
        "model.evaluate(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eNl2HraMK6YQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Proviamo a dialogare con il nostro Dante-bot, il testo che inseriremo verrà usato come seed per la generazione."
      ]
    },
    {
      "metadata": {
        "id": "MVMfMusoK_0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "be7a3fda-3e5d-4421-a3a1-9bf8865bfd4c"
      },
      "cell_type": "code",
      "source": [
        "seed = \"\"\n",
        "\n",
        "while(seed!=\"ciao\"):\n",
        "  seed = input(\"Io: \")\n",
        "  generated = generate(seed=seed)\n",
        "  print(\"Dante: \"+generated)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Io: Nel mezzo del cammin di nostra vita\n",
            "Dante: è voce ciò che nacque                                                                                                                                             \n",
            "Io: Ho smarrito la retta via\n",
            "Dante: e solea mè martiro sott lora che la foga mentr ïo presi e questi dirai ond io fossi inginocchiato e questi tonda fummi in render \n",
            "Io: Come sta Beatrice ?\n",
            "Dante: e dimmi di penter onde si tolse a le sue orme mi prese il mento in sù negando in sù venir vincendo la folle pinge \n",
            "Io: ciao\n",
            "Dante: lume mota condotto questo fascio luna e sieti lento ali ficca sì che la presente e l petto sì o volta cive a aspettar o \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}