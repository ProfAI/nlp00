{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glove.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/09%20-%20Word%20embedding%20e%20Word2Vec/glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkdUZTRZBP80",
        "colab_type": "text"
      },
      "source": [
        "# GloVe\n",
        "Il modello **GloVe (Global Vectors for Word Representation)** è un metodo non supervisionato per l'apprendimento della rappresentazione vettoriale delle parole. La prima versione di GloVe è stata creata nei laboratori di NLP di Standford nel 2014, i quali hanno messo a disposizione il modello pre-addestrato sull'intero corpus di testo di Wikipedia, con qualcosa come 6 miliardi di parole. In questo notebook utilizzeremo il modello pre-addestrato per creare lo strato di embedding di una rete neurale per classificare la polarità di recensioni di film utilizzando l'IMDB Movie Reviews Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgewpERwNiiS",
        "colab_type": "text"
      },
      "source": [
        "## Prepariamo i dati"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_xAZpjJqa5L",
        "colab_type": "text"
      },
      "source": [
        "In precedenza abbiamo visto come scaricare e preprocessare l'IMDB Movie Reviews Dataset, Keras mette a disposizione tale dataset già preprocessato.\n",
        "<br>**ATTENZIONE**<br>\n",
        "Se caricando il dataset ottieni questo errore:<br>\n",
        "*ValueError: Object arrays cannot be loaded when allow_pickle=False*\n",
        "<br>\n",
        "questo è casuato da un bug nell'ultima versione di keras, per correggerlo esegui il downgrade di numpy usando la cella di codice qui sotto e riavvia il kernel (su Colaboratory seleziona Runtime dalla barra dei comandi e clicca su Restart Runtime)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ixlnXETWRF",
        "colab_type": "code",
        "outputId": "c46b4d83-6508-48e0-eff0-077f26de3f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "!pip install numpy==1.16.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 8.1MB/s \n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.16.3\n",
            "    Uninstalling numpy-1.16.3:\n",
            "      Successfully uninstalled numpy-1.16.3\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT-U5zhnSZZG",
        "colab_type": "code",
        "outputId": "7ed53631-d198-476d-d9f5-a125bc5c6557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
        "\n",
        "print(X_train[0][:10])\n",
        "print(y_train[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hGPDrLVOGzY",
        "colab_type": "text"
      },
      "source": [
        "Ogni riga della lista con le features corrisponde ad una frase, ogni colonna contiene l'indice di una parola all'interno del vocabolario dell'intero corpus di testo. Il vettore con il target contiene un unico valore che può essere 0 per una recensione negativa o 1 per una recensione positiva.<br><br>\n",
        "Per rendere le features un buon input per il nostro modello dobbiamo fare in modo che ogni frase abbia la stessa lunghezza, per farlo possiamo usare la funzione *pad_sequences(text)* di keras, che riduce tutte le frasi ad una lunghezza prefissata troncando quelle troppo lunghe e aggiungendo degli zeri a quelle troppo brevi. Usiamo una lunghezza comune di 50 parole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkJLREGvPBtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3416432b-e957-4c2e-d32a-7de05d663f2d"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 50\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen = maxlen)\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6bjrSQOPsUr",
        "colab_type": "text"
      },
      "source": [
        "Adesso i dati sono pronti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScqnikSMPu1I",
        "colab_type": "text"
      },
      "source": [
        "## Carichiamo il modello GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0hKGO1-qK2J",
        "colab_type": "text"
      },
      "source": [
        "Possiamo scaricare il modello pre-addestrato [da questo link](http://nlp.stanford.edu/data/glove.6B.zip). Se utilizzi Google Colab o comunque hai wget installato sul tuo computer esegui pure la cella di codice qui sotto per scaricare il dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8d5wxXRBO5v",
        "colab_type": "code",
        "outputId": "8632c857-662c-4a04-fb33-9cf8ce735b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-03 14:13:05--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-05-03 14:13:11--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  3.54MB/s    in 3m 4s   \n",
            "\n",
            "2019-05-03 14:16:16 (4.46 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p9aAzJiqUwg",
        "colab_type": "text"
      },
      "source": [
        "Ed estraiamo il file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtvH2-kRRoHK",
        "colab_type": "code",
        "outputId": "ffcb85e6-82fe-403c-d694-54714962f53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKObUykSGrvX",
        "colab_type": "text"
      },
      "source": [
        "Lo zip contiene 4 file differenti, ognuno dei quali con un numero di dimensioni differente, 50, 100, 200 e 300. Leggiamo la prima riga del file con 50 dimensioni per comprendere come questo è strutturato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCMijSPqG0Rj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2da4ab90-773d-4378-b2a9-d3acd467669c"
      },
      "source": [
        "with open(\"glove.6B.50d.txt\") as file:\n",
        "  for line in file.readlines():\n",
        "    print(line)\n",
        "    break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KBuui-CGyWi",
        "colab_type": "text"
      },
      "source": [
        "Come vedi ogni riga corrisponde all'embedding per una determianta parola, il primo elemento della riga è la parola, i restanti solo i valori degli embedding. Alla luce di ciò definiamo una funzione per caricare gli embedding all'iterno di un dizionario la cui chiave è la parola. Usiamo il file con 100 dimensioni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MKOdv89OqgP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "c73c380f-2972-4497-ac3e-701313b6e54a"
      },
      "source": [
        "def load_embedding(filename):\n",
        "  \n",
        "  with open(filename) as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "  embedding = dict()\n",
        "  \n",
        "  for line in lines:\n",
        "    parts = line.split()\n",
        "    embedding[parts[0]] = np.asarray(parts[1:])\n",
        "    \n",
        "  return embedding\n",
        "\n",
        "raw_embedding = load_embedding('glove.6B.100d.txt')\n",
        "raw_embedding[\"man\"]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0.37293', '0.38503', '0.71086', '-0.65911', '-0.0010128',\n",
              "       '0.92715', '0.27615', '-0.056203', '-0.24294', '0.24632',\n",
              "       '-0.18449', '0.31398', '0.48983', '0.09256', '0.32958', '0.15056',\n",
              "       '0.57317', '-0.18529', '-0.52277', '0.46191', '0.92038',\n",
              "       '0.031001', '-0.16246', '-0.40567', '0.78621', '0.57722',\n",
              "       '-0.53501', '-0.68228', '0.16987', '0.3631', '-0.071773',\n",
              "       '0.47233', '0.027806', '-0.14951', '0.17543', '-0.37573',\n",
              "       '-0.78517', '0.58171', '0.86859', '0.031445', '-0.45897',\n",
              "       '-0.040917', '0.95897', '-0.16975', '0.13045', '0.27434',\n",
              "       '-0.069485', '0.022402', '0.24977', '-0.21536', '-0.32406',\n",
              "       '-0.39867', '0.68613', '1.7923', '-0.37848', '-2.2477', '-0.77025',\n",
              "       '0.46582', '1.2411', '0.57756', '0.41151', '0.84328', '-0.54259',\n",
              "       '-0.16715', '0.73927', '-0.093477', '0.90278', '0.50889',\n",
              "       '-0.50031', '0.26451', '0.15443', '-0.29432', '0.10906',\n",
              "       '-0.26667', '0.35438', '0.049079', '0.18018', '-0.5859',\n",
              "       '-0.55542', '-0.28987', '0.74278', '0.3453', '-0.028757',\n",
              "       '-0.22646', '-1.3113', '-0.5719', '-0.52306', '-0.1267',\n",
              "       '-0.098678', '-0.53463', '0.28607', '-0.37501', '0.45742',\n",
              "       '0.045975', '-0.24675', '0.045656', '-0.38302', '-0.93711',\n",
              "       '0.039138', '-0.53911'], dtype='<U10')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NpywazVH2hq",
        "colab_type": "text"
      },
      "source": [
        "Adesso dobbiamo trasformare il dizionario in una matrice, in cui ogni riga rappresenta contiene la rappresentazione vettoriale della parola che si trova alla stessa posizione all'interno del vocabolario dell'intero corpus di testo. Keras ci mette a disposizione anche il vocabolario già creato per questo dataset. possiamo ottenerlo con la funzione *.get_word_index()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44IpAdCRKFWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "413b34e4-03f2-4216-86e9-5b1352fcdbac"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "word_index[\"man\"]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNNE28PsKLaX",
        "colab_type": "text"
      },
      "source": [
        "Definiamo la funzione per creare la matrie ed utilizziamola."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doMODtsZReBJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66f4eb69-1107-400f-f46b-0d8161179aef"
      },
      "source": [
        "def get_weight_matrix(embedding, word_index):\n",
        "  \n",
        "  vocab_size = len(word_index)\n",
        "  \n",
        "  weight_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "  for word, i in word_index.items():\n",
        "    vector = embedding.get(word)\n",
        "    if vector is not None:\n",
        "      weight_matrix[i] = vector\n",
        "      \n",
        "  return weight_matrix\n",
        "\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, word_index)\n",
        "embedding_vectors.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88584, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y23OaHbKtbn",
        "colab_type": "text"
      },
      "source": [
        "# Creiamo la rete\n",
        "Cominciamo creando lo strato di embedding della rete, i primi due parametri saranno il numero di parole e il numero di dimensioni, passiamo gli embeddings già calcolati all'interno del parametro *weights* e settiamo il parametro *trainable* a False per indicare di non modificare questo strato della rete durante l'addestramento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va046uMMRegc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(len(word_index), 100, weights=[embedding_vectors], trainable=False, input_length=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVGu3IIhXuuw",
        "colab_type": "text"
      },
      "source": [
        "Ora creiamo la nostra rete neurale aggiungendo al primo strato lo strato di embedding appena creato, usiamo la classe *Flatten* di Keras per spacchettare la matrice con il word embedding in un unico vettore che unisce le righe della matrice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU90syz1RuVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3d4wQO9X0OB",
        "colab_type": "text"
      },
      "source": [
        "Compiliamo il modello ed avviamo l'addestramento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w3mIiB3QAUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1e91d545-eff7-41de-ae8d-3dd61f3a727f"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, epochs=10)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.7187 - acc: 0.5211\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.6685 - acc: 0.5871\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.6483 - acc: 0.6206\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.6326 - acc: 0.6402\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.6229 - acc: 0.6519\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.6143 - acc: 0.6626\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.6088 - acc: 0.6671\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.6020 - acc: 0.6756\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.5993 - acc: 0.6758\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 0s 6us/step - loss: 0.5934 - acc: 0.6829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc390f81c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSNuihGZhBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ad2ad169-2921-4ac7-878b-da1cf3cc621a"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 39us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6988005238342285, 0.57212]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIhX1KUJZmU_",
        "colab_type": "text"
      },
      "source": [
        "Lo strato di embedding funziona correttamente, ma il risultato è piuttosto scarso, perché ? Perché utilizzare l'embedding senza tener conto della relazione temporale all'interno di una sequenza (come può essere una frase) è una cosa abbastanza inutile. Per tener conto delle informazioni sequenziali dobbiamo usare degli strati ricorrenti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrgQfvmZZZuy",
        "colab_type": "text"
      },
      "source": [
        "# Usiamo uno Strato Ricorrente\n",
        "Le Reti Neurali Ricorrenti (RNN) saranno l'argomento della prossima sezione, qui vediamo semplicemente come varia il risultato utilizzandole insieme all'embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q5e8lyHRQ4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(num_words, 50))\n",
        "model.add(embedding_layer)\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rf5PVihdUnD",
        "colab_type": "text"
      },
      "source": [
        "Come algoritmo di ottimizzazione usiamo l'RMSProp che dovrebbe portare a risultati migliori per le reti ricorrenti, siccome è più lento a convergere rispetto all'ADAM aumentiamo il numero di epoche dell'addestramento a 20."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fDXsPsCWd0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "12ce90c6-80f5-4fd1-d29c-e96e62d7ddc3"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, epochs=20)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 0.7050 - acc: 0.5268\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 1s 56us/step - loss: 0.6844 - acc: 0.5509\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 1s 57us/step - loss: 0.6764 - acc: 0.5638\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 1s 57us/step - loss: 0.6702 - acc: 0.5730\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 1s 58us/step - loss: 0.6648 - acc: 0.5824\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 1s 58us/step - loss: 0.6593 - acc: 0.5920\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 1s 57us/step - loss: 0.6548 - acc: 0.5954\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 1s 57us/step - loss: 0.6504 - acc: 0.6047\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6471 - acc: 0.6115\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6436 - acc: 0.6166\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6415 - acc: 0.6156\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 1s 50us/step - loss: 0.6376 - acc: 0.6260\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6349 - acc: 0.6268\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 1s 52us/step - loss: 0.6322 - acc: 0.6312\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6303 - acc: 0.6330\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6269 - acc: 0.6380\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6259 - acc: 0.6396\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 1s 50us/step - loss: 0.6227 - acc: 0.6410\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6224 - acc: 0.6416\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 1s 51us/step - loss: 0.6194 - acc: 0.6462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc385fedb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbWVLQyaWkN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ccd9ddef-233d-4c36-b9ca-a271a6380cfd"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 13s 520us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6410095851516724, 0.62128]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    }
  ]
}